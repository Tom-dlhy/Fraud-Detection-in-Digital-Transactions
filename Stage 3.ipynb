{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 - Aiming Perfection With Deep Learning \n",
    "### For Stage 3 of the project, even though we've achieved near-perfect results with traditional machine learning algorithms, we aim to attain absolute perfection by harnessing neural networks. This will enable us to explore more complex architectures like fully connected networks (MLPs), pushing the boundaries of performance to achieve even higher precision and recall in fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Useful steps from stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('cleaned_data.csv', sep=',')\n",
    "cleaned_data.head()\n",
    "\n",
    "X = cleaned_data.drop(columns=['fraud'])\n",
    "y = cleaned_data['fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "def remove_outliers_zscore(data, columns, threshold=3):\n",
    "    \"\"\"\n",
    "    Removes outliers from the specified columns using the Z-score method.\n",
    "\n",
    "    Parameters:\n",
    "    - data : pandas DataFrame\n",
    "        The input DataFrame containing the data.\n",
    "    - columns : list\n",
    "        List of numeric columns to check for outliers.\n",
    "    - threshold : float, optional (default: 3)\n",
    "        The Z-score threshold to identify outliers. Values above this threshold\n",
    "        (or below -threshold) will be considered as outliers.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame : A new DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    df = data.copy() # we create a copy of the input DataFrame to prevent memory issues\n",
    "\n",
    "    for col in columns:\n",
    "        df['zscore'] = zscore(df[col])\n",
    "        df = df[(df['zscore'].abs() < threshold)]\n",
    "\n",
    "    df = df.drop(columns=['zscore'])\n",
    "    return df\n",
    "\n",
    "columns_to_check = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price'] # we of course only care about non-categorical columns as categorical columns cannot have outliers.\n",
    "X_train_clean = remove_outliers_zscore(X_train, columns_to_check)\n",
    "y_train_clean = y_train.loc[X_train_clean.index] # we need to make sure to only keep the labels corresponding to the cleaned data\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test) # we only transform the test data, we do not fit the scaler again to avoid data leakage.\n",
    "\n",
    "\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not use SMOTE with neural networks because it can lead to overfitting by introducing synthetic examples that add noise and redundancy, which affects the model's ability to generalize effectively. Neural networks models already have a great learning ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIABasics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
